import { OllamaModel } from '../types';

/**
 * Multi-Provider Chat Service with Automatic Fallback
 *
 * Priority Chain:
 * 1. Ollama kimi-k2:1t-cloud (Primary)
 * 2. Ollama qwen-coder:480b-cloud (Secondary)
 * 3. Google Gemini gemini-2.0-flash-exp (Tertiary)
 * 4. OpenAI gpt-4o (Quaternary)
 *
 * Features:
 * - Automatic failover if a provider is unavailable
 * - Conversation history management
 * - Vision analysis support
 * - Web search with grounding sources (Gemini only)
 */

const API_URL = import.meta.env.DEV ? 'http://localhost:3000' : '';

interface Message {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface ChatOptions {
  model?: string;
  temperature?: number;
  topP?: number;
  maxTokens?: number;
  enableFallback?: boolean;
}

type Provider = 'ollama' | 'gemini' | 'openai';

/**
 * Provider configuration with models
 */
const PROVIDER_CONFIG = {
  ollama: {
    primary: 'kimi-k2:1t-cloud',
    secondary: 'qwen-coder:480b-cloud',
    vision: 'llava:34b',
  },
  gemini: {
    primary: 'gemini-2.0-flash-exp',
    vision: 'gemini-2.0-flash-exp',
  },
  openai: {
    primary: 'gpt-4o',
    vision: 'gpt-4o',
  },
};

/**
 * Chat class with automatic provider failover
 */
export class Chat {
  private history: Message[] = [];
  private temperature: number;
  private topP: number;
  private maxTokens: number;
  private enableFallback: boolean;
  private currentProvider: Provider | null = null;

  constructor(options: ChatOptions = {}) {
    this.temperature = options.temperature ?? 0.7;
    this.topP = options.topP ?? 1.0;
    this.maxTokens = options.maxTokens ?? 2048;
    this.enableFallback = options.enableFallback ?? true;
  }

  /**
   * Send a message with automatic fallback
   */
  async sendMessage(userMessage: string): Promise<string> {
    this.history.push({
      role: 'user',
      content: userMessage,
    });

    const providers: Array<{ provider: Provider; model: string }> = [
      { provider: 'ollama', model: PROVIDER_CONFIG.ollama.primary },
      { provider: 'ollama', model: PROVIDER_CONFIG.ollama.secondary },
      { provider: 'gemini', model: PROVIDER_CONFIG.gemini.primary },
      { provider: 'openai', model: PROVIDER_CONFIG.openai.primary },
    ];

    for (let i = 0; i < providers.length; i++) {
      const { provider, model } = providers[i];

      try {
        console.log(`ğŸ”„ Trying ${provider} (${model})...`);

        const response = await this.callProvider(provider, model);

        console.log(`âœ… Success with ${provider}`);
        this.currentProvider = provider;

        // Add assistant response to history
        this.history.push({
          role: 'assistant',
          content: response,
        });

        return response;
      } catch (error) {
        console.log(`âš ï¸ ${provider} failed:`, error instanceof Error ? error.message : 'Unknown error');

        if (!this.enableFallback || i === providers.length - 1) {
          throw error;
        }

        console.log(`â†ªï¸ Falling back to next provider...`);
      }
    }

    throw new Error('All AI providers failed');
  }

  /**
   * Call a specific provider
   */
  private async callProvider(provider: Provider, model: string): Promise<string> {
    const response = await fetch(`${API_URL}/api/unified`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        messages: this.history,
        model,
        temperature: this.temperature,
        top_p: this.topP,
        maxTokens: this.maxTokens,
        preferredProvider: provider,
        enableFallback: false, // We handle fallback at this level
      }),
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw new Error(errorData.error || `HTTP error! status: ${response.status}`);
    }

    const data = await response.json();
    return data.choices[0].message.content;
  }

  /**
   * Get conversation history
   */
  getHistory(): Message[] {
    return [...this.history];
  }

  /**
   * Clear conversation history
   */
  clearHistory(): void {
    this.history = [];
    this.currentProvider = null;
  }

  /**
   * Set system instruction
   */
  setSystemInstruction(instruction: string): void {
    this.history = this.history.filter(msg => msg.role !== 'system');
    this.history.unshift({
      role: 'system',
      content: instruction,
    });
  }

  /**
   * Get the current provider being used
   */
  getCurrentProvider(): Provider | null {
    return this.currentProvider;
  }
}

/**
 * Perform web search with automatic fallback
 * Priority: Gemini (has native search) > Ollama > OpenAI
 */
export async function performSearch(
  query: string,
  options: ChatOptions = {}
): Promise<{ content: string; sources?: any[] }> {
  // Try Gemini first (has native web search with sources)
  const providers: Array<{ provider: Provider; model: string; hasSearch: boolean }> = [
    { provider: 'gemini', model: PROVIDER_CONFIG.gemini.primary, hasSearch: true },
    { provider: 'ollama', model: PROVIDER_CONFIG.ollama.primary, hasSearch: false },
    { provider: 'ollama', model: PROVIDER_CONFIG.ollama.secondary, hasSearch: false },
    { provider: 'openai', model: PROVIDER_CONFIG.openai.primary, hasSearch: false },
  ];

  for (let i = 0; i < providers.length; i++) {
    const { provider, model, hasSearch } = providers[i];

    try {
      console.log(`ğŸ”„ Trying search with ${provider}...`);

      const searchPrompt = hasSearch
        ? `Search the web and provide information about: ${query}`
        : `Please provide information about: ${query}`;

      const response = await fetch(`${API_URL}/api/unified`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          messages: [{ role: 'user', content: searchPrompt }],
          model,
          temperature: options.temperature ?? 0.7,
          top_p: options.topP ?? 1.0,
          maxTokens: options.maxTokens ?? 2048,
          preferredProvider: provider,
          enableFallback: false,
        }),
      });

      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }

      const data = await response.json();
      console.log(`âœ… Search successful with ${provider}`);

      return {
        content: data.choices[0].message.content,
        sources: data.groundingMetadata?.groundingChunks, // Only available for Gemini
      };
    } catch (error) {
      console.log(`âš ï¸ Search failed with ${provider}:`, error instanceof Error ? error.message : 'Unknown error');

      if (!options.enableFallback || i === providers.length - 1) {
        throw error;
      }

      console.log(`â†ªï¸ Falling back to next provider...`);
    }
  }

  throw new Error('All AI providers failed for search');
}

/**
 * Analyze image with automatic fallback
 * Priority: Gemini (best vision) > OpenAI > Ollama
 */
export async function analyzeImage(
  imageData: string,
  prompt: string,
  options: ChatOptions = {}
): Promise<string> {
  const providers: Array<{ provider: Provider; model: string }> = [
    { provider: 'gemini', model: PROVIDER_CONFIG.gemini.vision },
    { provider: 'openai', model: PROVIDER_CONFIG.openai.vision },
    { provider: 'ollama', model: PROVIDER_CONFIG.ollama.vision },
  ];

  for (let i = 0; i < providers.length; i++) {
    const { provider, model } = providers[i];

    try {
      console.log(`ğŸ”„ Trying image analysis with ${provider}...`);

      const response = await fetch(`${API_URL}/api/unified`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          messages: [
            {
              role: 'user',
              content: [
                { type: 'text', text: prompt },
                { type: 'image_url', image_url: { url: imageData } },
              ],
            },
          ],
          model,
          temperature: options.temperature ?? 0.7,
          top_p: options.topP ?? 1.0,
          maxTokens: options.maxTokens ?? 2048,
          preferredProvider: provider,
          enableFallback: false,
        }),
      });

      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }

      const data = await response.json();
      console.log(`âœ… Image analysis successful with ${provider}`);

      return data.choices[0].message.content;
    } catch (error) {
      console.log(`âš ï¸ Image analysis failed with ${provider}:`, error instanceof Error ? error.message : 'Unknown error');

      if (!options.enableFallback || i === providers.length - 1) {
        throw error;
      }

      console.log(`â†ªï¸ Falling back to next provider...`);
    }
  }

  throw new Error('All AI providers failed for image analysis');
}

/**
 * Analyze text with automatic fallback
 */
export async function analyzeText(
  text: string,
  instruction: string,
  options: ChatOptions = {}
): Promise<string> {
  const chat = new Chat(options);
  chat.setSystemInstruction(instruction);
  return await chat.sendMessage(text);
}

/**
 * Generate lesson ideas with automatic fallback
 */
export async function generateLessonIdeas(
  topic: string,
  ageGroup: string,
  persona: {
    name: string;
    title: string;
    bio: string;
    expertise: string[];
  },
  options: ChatOptions = {}
): Promise<string> {
  const prompt = `
    **è§’è‰²æ‰®æ¼”æƒ…å¢ƒ:**
    ä½ æ˜¯ä¸€ä½æ‰®æ¼” **${persona.name}** çš„ AI åŠ©ç†ï¼Œæ˜¯ä¸€ä½å°ˆç‚ºç‚ºä¸»æ—¥å­¸è€å¸«æœå‹™çš„èª²ç¨‹è¨­è¨ˆå°ˆå®¶ã€‚
    ä½ çš„è§’è‰²è¨­å®š:
    - **å§“å:** ${persona.name}
    - **è·ç¨±:** ${persona.title}
    - **ç°¡ä»‹/ç†å¿µ:** ${persona.bio}
    - **å°ˆé•·:** ${persona.expertise.join(', ')}

    **æƒ…å¢ƒ:**
    - **åœ°å€:** æ±äº / æ±å—äº
    - **å¹´ä»½:** 2026
    - **å°è±¡:** ä¸€ä½æ­£åœ¨å‚™èª²çš„ä¸»æ—¥å­¸è€å¸«ã€‚
    - **ä½ çš„ç›®æ¨™:** æ‰®æ¼”ä¸€ä½å¯Œæœ‰åŒç†å¿ƒå’Œå‰µé€ åŠ›çš„å°å¸«ã€‚ä½ çš„é»å­å¿…é ˆèˆ‡æŒ‡å®šçš„åœ°å€å’Œæ™‚é–“å…·æœ‰æ–‡åŒ–å’Œæƒ…å¢ƒé—œè¯æ€§ã€‚è€ƒæ…®å¯èƒ½å½±éŸ¿å­¸ç”Ÿçš„ç•¶åœ°äº‹ä»¶ã€ç§‘æŠ€ç´ é¤Šå’Œç¶“æ¿Ÿç‹€æ³ã€‚

    **ä»»å‹™:**
    æ ¹æ“šä½¿ç”¨è€…çš„è«‹æ±‚ï¼Œç‚ºèª²ç¨‹ç”Ÿæˆå¯Œæœ‰å‰µæ„å’Œå¸å¼•åŠ›çš„é»å­ã€‚

    **è€å¸«çš„è¦æ±‚:**
    - **ç›®æ¨™å¹´é½¡å±¤:** ${ageGroup}
    - **èª²ç¨‹ä¸»é¡Œ:** ${topic}

    **ä½ çš„å›è¦†å¿…é ˆåŒ…å«:**
    1.  **é–‹å ´æ´»å‹•/ç ´å†°éŠæˆ²:** ä¸€å€‹ç°¡çŸ­æœ‰è¶£çš„æ´»å‹•ï¼Œç”¨æ–¼èª²ç¨‹é–‹å§‹ä¸¦ä»‹ç´¹ä¸»é¡Œã€‚æ´»å‹•æ‡‰è©²æ˜¯ä½æˆæœ¬ä¸”å®¹æ˜“æº–å‚™çš„ã€‚
    2.  **é—œéµè¨è«–å•é¡Œ:** é‡å°æŒ‡å®šå¹´é½¡å±¤è¨­è¨ˆçš„ç™¼äººæ·±çœçš„å•é¡Œï¼Œä»¥ä¿ƒé€²ç†è§£å’Œæ‡‰ç”¨ã€‚å•é¡Œçš„è¨­è¨ˆæ‡‰é¼“å‹µé–‹æ”¾æ€§è¨è«–ã€‚
    3.  **ä¸»è¦æ´»å‹•/æ‰‹å·¥è—:** ä¸€å€‹å‹•æ‰‹åšçš„æ´»å‹•ã€æ‰‹å·¥è—æˆ–å°çµ„é …ç›®ï¼Œä»¥åŠ å¼·èª²ç¨‹çš„ä¸»è¦æ€æƒ³ã€‚å»ºè­°ç‚ºä¸åŒè³‡æºæ°´å¹³æä¾›é¸é …ï¼ˆä¾‹å¦‚ï¼Œåƒ…ç”¨ç´™å¼µçš„ç°¡å–®ç‰ˆæœ¬ï¼Œæˆ–åœ¨ææ–™å……è¶³æƒ…æ³ä¸‹çš„è¼ƒè¤‡é›œç‰ˆæœ¬ï¼‰ã€‚
    4.  **çµæŸç¦±å‘Š/ç¸½çµ:** ä¸€å€‹ç°¡å–®çš„çµæŸæ€æƒ³æˆ–ç¦±å‘Šå»ºè­°ï¼Œå°‡èª²ç¨‹èˆ‡å­¸ç”Ÿçš„æ—¥å¸¸ç”Ÿæ´»è¯ç¹«èµ·ä¾†ã€‚

    **æ ¼å¼è¦æ±‚:**
    - ä½¿ç”¨æ¸…æ™°çš„ Markdown æ ¼å¼ï¼ˆæ¨™é¡Œã€ç²—é«”ã€åˆ—è¡¨ï¼‰ã€‚
    - ä»¥æº«æš–ã€é¼“å‹µå’Œå°å¸«èˆ¬çš„èªæ°£æ›¸å¯«ï¼Œèˆ‡ä½ çš„è§’è‰²è¨­å®šä¿æŒä¸€è‡´ã€‚
  `;

  const chat = new Chat({
    temperature: options.temperature ?? 0.7,
    topP: options.topP ?? 1.0,
    maxTokens: options.maxTokens ?? 2048,
    enableFallback: options.enableFallback ?? true,
  });

  return await chat.sendMessage(prompt);
}

/**
 * Test provider availability
 */
export async function testProviders(): Promise<{
  ollama: boolean;
  gemini: boolean;
  openai: boolean;
}> {
  const results = {
    ollama: false,
    gemini: false,
    openai: false,
  };

  const testMessage = 'Hello, respond with OK';

  // Test Ollama
  try {
    const response = await fetch(`${API_URL}/api/unified`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        messages: [{ role: 'user', content: testMessage }],
        model: PROVIDER_CONFIG.ollama.primary,
        preferredProvider: 'ollama',
        enableFallback: false,
        maxTokens: 10,
      }),
    });
    results.ollama = response.ok;
  } catch {
    results.ollama = false;
  }

  // Test Gemini
  try {
    const response = await fetch(`${API_URL}/api/unified`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        messages: [{ role: 'user', content: testMessage }],
        model: PROVIDER_CONFIG.gemini.primary,
        preferredProvider: 'gemini',
        enableFallback: false,
        maxTokens: 10,
      }),
    });
    results.gemini = response.ok;
  } catch {
    results.gemini = false;
  }

  // Test OpenAI
  try {
    const response = await fetch(`${API_URL}/api/unified`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        messages: [{ role: 'user', content: testMessage }],
        model: PROVIDER_CONFIG.openai.primary,
        preferredProvider: 'openai',
        enableFallback: false,
        maxTokens: 10,
      }),
    });
    results.openai = response.ok;
  } catch {
    results.openai = false;
  }

  return results;
}

export default {
  Chat,
  performSearch,
  analyzeImage,
  analyzeText,
  generateLessonIdeas,
  testProviders,
};
